title="Paxos series: optimizing Paxos"
publication_date="2024-07-29T20:29:19.769717767+00:00"
language="en"
---- sadraskol ----

So far, our Paxos algorithm consisted of at least 3 messages for every client requests.
One Round Trip Time (RTT) from the client to the acceptor,
one RTT for phase 1 and one RTT for phase 2.
Intuitively, failure do not happen a every client requests.
Let's explore opportunities to "optimize" Paxos.

## Observations

First, Paxos is designed for the worst-case scenario: every node can be failing at every time.
The cost of the protocol is quite high even for the best-case scenario.
Most failures in distributed systems are rare and even rarely chaotic.
It would be a reasonable compromise to have more performant best-case scenario while have more expensive worst-case scenario.

Let's observe the role of phase 1: making sure there are no conflicting proposal.
To avoid conflicts, we can choose to have a single proposer at a time.
Having a single proposer at a time is also named a leader.

## Leader Election

Electing a leader in Paxos protocol adds two problems:

1. Clients need to know who is the current leader
2. If the leader fails, an election must take place

The first issue is fixed if non-leader nodes refuse requests and redirect clients to their choosen leader.

Leader election requires two components: a failure detector and a leader election protocol.

Failure detector are usually implemented with timeouts and healthchecks.
Each node expect their leader to send them a healthcheck at regular interval.
If a node does not receive a healthcheck past a certain timeout it triggers the leader election protocol.
You can find more information on failure detection in the foundation [paper by Chandra and Toueg](http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf).

The leader election problem can be reduced down to... a consensus problem!
This is where using Paxos makes sense.
Since it is a rare event that can get worst, using Paxos protocol is a safe choice here.

When applying leader election to the Paxos + log algorithm we covered in the last article, one must be careful.
The protocol must make sure there are no two leaders at the same time.
This can lead to a "split-brain" scenario that defeats the goal of consensus in the first place.

## Raft 

Paxos requires a lot of knowledge.
First, you need to understand the single decree version.
Each phase have a specific role, and it's demanding to understand the phase 1.
Correctness and progress is not obvious.
Many implementation details are left to the reader and can be



## Other optimizations

1. Specific optimization

2. Generalized optimization

No can be explored in this rich [paper](https://arxiv.org/pdf/2012.15762.pdf).

## TODO

Most explanations mix up the SMR and the consensus problem.
It makes deep understanding really difficult.
What I mean by deep understanding is: understanding to the point of being comfortable at implementing the protocol.
Stop doing that.

Reading in Paxos in 1 RTT is confusing.
A client can attempt to chose a safe value in one RTT.
But if no value is yet safe, you are back to use another RTT...
If you implement SQL transactions with Paxos, `SELECT` is just another Paxos decision (with 2 RTT).

Is Paxos reading useless?
No you can use it to recover the log from the leader failure, without voting for another leader!
It's like running the phase 1, but knowing that the safe value can be decided!

MegaStore uses separate log replication and SMR execution.

Leaders are proposers chosen in pre-vote.
Raft uses a clever trick: once the leader is choosen, only he can be proposer.
It removes the need for phase 1.

Cloudflare issue with etcd (raft based) has a solution [Heidi Howard suggested](https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/).
It is easier to understand the solution if you understand phase 1 of Paxos.
